{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3841aa",
   "metadata": {},
   "source": [
    "# XGBoost Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d293ce",
   "metadata": {},
   "source": [
    "**XGBoost** (eXtreme Gradient Boosting) is a highly optimized and scalable **ensemble machine learning algorithm** that uses the **gradient boosted decision tree** framework. It is widely known for its exceptional speed, performance, and ability to handle large, complex datasets, often winning machine learning competitions like those on Kaggle. \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è How XGBoost Works (The Boosting Concept)\n",
    "\n",
    "XGBoost is an advanced implementation of the **Boosting** ensemble technique. Unlike **Random Forest**, where trees are built independently (bagging), XGBoost builds trees **sequentially** and iteratively to correct the errors of the previous trees.\n",
    "\n",
    "1.  **Initial Prediction:** The process starts with a simple initial prediction (often a constant value).\n",
    "2.  **Calculate Residuals (Errors):** The difference (residual) between the predicted value and the actual target value is calculated.\n",
    "3.  **Train a New Tree:** A new, simple decision tree (a \"weak learner\") is trained specifically to predict these **residuals**.\n",
    "4.  **Update the Model:** The prediction of this new tree is added to the initial model's prediction, and the process is repeated. Each subsequent tree learns from the **errors** of the **cumulative model** up to that point.\n",
    "5.  **Final Prediction:** The final prediction is the **sum** of the initial prediction and the weighted predictions of all the sequentially built trees.\n",
    "\n",
    "This sequential process allows the model to continuously focus on the data points that were previously difficult to classify, turning a sequence of weak learners into a single, strong predictive model.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Key Optimization Features\n",
    "\n",
    "The \"eXtreme\" in XGBoost refers to its performance optimizations and architectural enhancements compared to traditional gradient boosting:\n",
    "\n",
    "* **Regularization:** It includes $\\text{L}1$ (Lasso) and $\\text{L}2$ (Ridge) **regularization** terms in its objective function. This penalizes complex models, which helps prevent **overfitting** and improves generalization.\n",
    "* **Parallel Processing:** Although the trees are built sequentially, XGBoost utilizes **parallelization** within the tree-building process (for split-finding) to significantly speed up training time.\n",
    "* **Handling Missing Data:** It has a built-in routine to handle **sparse data** and **missing values** by automatically learning the best direction for a split when a value is absent.\n",
    "* **Tree Pruning:** It uses a complexity parameter ($\\gamma$) to prune trees **after** they are fully grown, which is a more robust technique to combat overfitting than stopping the tree growth prematurely.\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö XGBoost vs. Random Forest\n",
    "\n",
    "| Feature | XGBoost (Boosting) | Random Forest (Bagging) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Tree Construction** | Trees are built **sequentially**, correcting the errors of the preceding trees. | Trees are built **in parallel** and independently from random data subsets. |\n",
    "| **Focus** | Primarily focuses on **reducing bias** (error) by learning from mistakes. | Primarily focuses on **reducing variance** by averaging independent results. |\n",
    "| **Accuracy** | Often achieves **higher predictive accuracy**, especially on large or heterogeneous datasets. | Generally **highly accurate**, but can sometimes overfit noisy data. |\n",
    "| **Overfitting** | Highly resistant due to built-in **regularization** and careful tree pruning. | Resistant, but less control over individual tree complexity. |\n",
    "| **Performance** | **Highly optimized** for speed and scalability (eXtreme) on large datasets. | Efficient, but not as optimized for distributed computing as XGBoost. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ecd075",
   "metadata": {},
   "source": [
    "f(x) ‚âà y\\\n",
    "\\\n",
    "f(x) = t1(x) + t2(x) + ... + tn(x)\\\n",
    "\\\n",
    "t1(x ‚âà y\\\n",
    "\\\n",
    "t2(x) ‚âà y - t1(x)\\\n",
    "\\\n",
    "t3(x) ‚âà y - t1(x) - t2(x)\\\n",
    "\\\n",
    "Œ£1 = t1(x)-y\\\n",
    "\\\n",
    "Œ£2 = t2(x) - Œ£1\\\n",
    "\\\n",
    "$\\Sigma_3$ = $t_3(x) - Œ£_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a186ad",
   "metadata": {},
   "source": [
    "# Attempt 1\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a18583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
