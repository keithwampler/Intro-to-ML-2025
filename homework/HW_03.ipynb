{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f619e285",
   "metadata": {},
   "source": [
    "# Homework 02\n",
    "## Keith Wampler\n",
    "#### AI Tool Used: Gemini 2.5 Pro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27aecf",
   "metadata": {},
   "source": [
    "### Task 1 - NRI Data Cleaning\n",
    "#### 1. Import the NRI data. Ensure that the FIPS code variable (‘STCOFIPS’) is correctly identified as a string / character variable. Otherwise, the leading zeros will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f78ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OID_  NRI_ID    STATE STATEABBRV  STATEFIPS   COUNTY COUNTYTYPE  \\\n",
      "0     1  C01001  Alabama         AL          1  Autauga     County   \n",
      "1     2  C01003  Alabama         AL          1  Baldwin     County   \n",
      "2     3  C01005  Alabama         AL          1  Barbour     County   \n",
      "3     4  C01007  Alabama         AL          1     Bibb     County   \n",
      "4     5  C01009  Alabama         AL          1   Blount     County   \n",
      "\n",
      "   COUNTYFIPS STCOFIPS  POPULATION  ...  WNTW_EALS            WNTW_EALR  \\\n",
      "0           1    01001       58764  ...  15.784587             Very Low   \n",
      "1           3    01003      231365  ...  56.205509  Relatively Moderate   \n",
      "2           5    01005       25160  ...  18.632002       Relatively Low   \n",
      "3           7    01007       22239  ...  13.308573             Very Low   \n",
      "4           9    01009       58992  ...  23.645930       Relatively Low   \n",
      "\n",
      "      WNTW_ALRB     WNTW_ALRP     WNTW_ALRA WNTW_ALR_NPCTL    WNTW_RISKV  \\\n",
      "0  2.687716e-07  7.410082e-09  8.725777e-06      10.461158   8494.906508   \n",
      "1  1.268231e-09  2.287120e-08  1.548360e-07      13.339523  65619.701638   \n",
      "2  5.788050e-07  2.347236e-08  7.606598e-07      16.125039  15501.730335   \n",
      "3  9.014679e-07  1.270300e-08  1.202015e-05      16.991643   7496.186940   \n",
      "4  5.268425e-07  1.482016e-08  2.002965e-07      12.039616  17175.160729   \n",
      "\n",
      "   WNTW_RISKS      WNTW_RISKR     NRI_VER  \n",
      "0   12.217626        Very Low  March 2023  \n",
      "1   52.083996  Relatively Low  March 2023  \n",
      "2   19.535476        Very Low  March 2023  \n",
      "3   11.104041        Very Low  March 2023  \n",
      "4   21.444480        Very Low  March 2023  \n",
      "\n",
      "[5 rows x 465 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# path to the NRI file\n",
    "nri_path = '/Users/keith/Documents/code/Intro to ML 2025/data/raw/NRI_Table_Counties.csv'\n",
    "\n",
    "# import the data w/ FIPS column as a string type\n",
    "nri_df = pd.read_csv(nri_path, dtype={'STCOFIPS': str})\n",
    "\n",
    "# check\n",
    "print(nri_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa28f0",
   "metadata": {},
   "source": [
    "#### 2. Subset the NRI data to include only the 5-digit state/county FIPS code and all colums ending with ‘_AFREQ’ and ‘_RISKR’. Each of these columns represents a different hazard type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effcc954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  STCOFIPS  AVLN_AFREQ      AVLN_RISKR  CFLD_AFREQ      CFLD_RISKR  \\\n",
      "0    01001         NaN  Not Applicable         NaN  Not Applicable   \n",
      "1    01003         NaN  Not Applicable    3.684142  Relatively Low   \n",
      "2    01005         NaN  Not Applicable         NaN  Not Applicable   \n",
      "3    01007         NaN  Not Applicable         NaN  Not Applicable   \n",
      "4    01009         NaN  Not Applicable         NaN  Not Applicable   \n",
      "\n",
      "   CWAV_AFREQ CWAV_RISKR  DRGT_AFREQ           DRGT_RISKR  ERQK_AFREQ  ...  \\\n",
      "0         0.0  No Rating   25.969774       Relatively Low    0.000431  ...   \n",
      "1         0.0  No Rating   12.353442  Relatively Moderate    0.000338  ...   \n",
      "2         0.0  No Rating   43.956953       Relatively Low    0.000227  ...   \n",
      "3         0.0  No Rating   28.894501             Very Low    0.000790  ...   \n",
      "4         0.0  No Rating   28.152598       Relatively Low    0.000817  ...   \n",
      "\n",
      "  TRND_AFREQ           TRND_RISKR TSUN_AFREQ         TSUN_RISKR VLCN_AFREQ  \\\n",
      "0   0.480184  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "1   0.953140  Relatively Moderate        NaN  Insufficient Data        NaN   \n",
      "2   0.739018  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "3   0.586160  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "4   0.710332  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "\n",
      "       VLCN_RISKR WFIR_AFREQ           WFIR_RISKR WNTW_AFREQ      WNTW_RISKR  \n",
      "0  Not Applicable   0.000035             Very Low   0.433437        Very Low  \n",
      "1  Not Applicable   0.002229  Relatively Moderate   0.182759  Relatively Low  \n",
      "2  Not Applicable   0.000038             Very Low   0.185759        Very Low  \n",
      "3  Not Applicable   0.000040             Very Low   0.743034        Very Low  \n",
      "4  Not Applicable   0.000035             Very Low   0.866873        Very Low  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# find columns ending with '_AFREQ' or '_RISKR'\n",
    "selected_cols = ['STCOFIPS'] + [col for col in nri_df.columns if col.endswith('_AFREQ') or col.endswith('_RISKR')]\n",
    "\n",
    "# create the subset\n",
    "nri_subset = nri_df[selected_cols]\n",
    "\n",
    "# check\n",
    "print(nri_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3e358",
   "metadata": {},
   "source": [
    "#### 3. Create a table / dataframe that, for each hazard type, shows the number of missing values in the ‘_AFREQ’ and ‘_RISKR’ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34334a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AFREQ_missing  RISKR_missing\n",
      "AVLN           3023              0\n",
      "CFLD           2646              0\n",
      "CWAV              0              0\n",
      "DRGT              7              0\n",
      "ERQK              0              0\n",
      "HAIL              7              0\n",
      "HRCN            918              0\n",
      "HWAV              0              0\n",
      "ISTM            229              0\n",
      "LNDS             40              0\n",
      "LTNG            123              0\n",
      "RFLD              0              0\n",
      "SWND              7              0\n",
      "TRND              7              0\n",
      "TSUN           3103              0\n",
      "VLCN           3125              0\n",
      "WFIR             88              0\n",
      "WNTW              0              0\n"
     ]
    }
   ],
   "source": [
    "# extract unique hazard prefixes\n",
    "hazards = sorted(list(set([col.split('_')[0] for col in nri_subset.columns if '_' in col])))\n",
    "\n",
    "# Create a dictionary to store missing value counts\n",
    "missing_counts = {}\n",
    "\n",
    "for hazard in hazards:\n",
    "    afreq_col = f\"{hazard}_AFREQ\"\n",
    "    riskr_col = f\"{hazard}_RISKR\"\n",
    "    missing_counts[hazard] = {\n",
    "        'AFREQ_missing': nri_subset[afreq_col].isnull().sum(),\n",
    "        'RISKR_missing': nri_subset[riskr_col].isnull().sum()\n",
    "    }\n",
    "\n",
    "# convert the dictionary to a df printing\n",
    "missing_df = pd.DataFrame.from_dict(missing_counts, orient='index')\n",
    "\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d420741",
   "metadata": {},
   "source": [
    "#### 4. Create a new column in the original data table indicating whether or not ‘AVLN_AFREQ’ is missing or observed. Show the cross-tabulation of the ‘AVLN_AFREQ’ missingness and ‘AVLN_RISKR’ columns (including missing values). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca538b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVLN_RISKR          Not Applicable  Relatively High  Relatively Low  \\\n",
      "AVLN_AFREQ_missing                                                    \n",
      "False                            0               15              52   \n",
      "True                          3023                0               0   \n",
      "\n",
      "AVLN_RISKR          Relatively Moderate  Very High  Very Low  \n",
      "AVLN_AFREQ_missing                                            \n",
      "False                                33          9        99  \n",
      "True                                  0          0         0  \n"
     ]
    }
   ],
   "source": [
    "# create a new column that is True if AVLN_AFREQ is null, and False otherwise\n",
    "nri_df['AVLN_AFREQ_missing'] = nri_df['AVLN_AFREQ'].isnull()\n",
    "\n",
    "# make cross-tabulation table\n",
    "# dropna=False ensures we see counts of missing values in the RISKR column as well\n",
    "cross_tab = pd.crosstab(\n",
    "    nri_df['AVLN_AFREQ_missing'],\n",
    "    nri_df['AVLN_RISKR'],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef40a3",
   "metadata": {},
   "source": [
    "#### Observation: AVLN_AFREQ is missing in every single case where AVLN_RISKR is \"Not Applicable\". This suggests that when an avalanche risk isn't applicable to a county (e.g., Florida), its frequency isn't recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3d54c",
   "metadata": {},
   "source": [
    "#### 5. Assuming that a risk that is “not applicable” to a county has an annualized frequency of 0, impute the relevant missing values in the ‘_AFREQ’ columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5609f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values in AVLN_AFREQ after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# make list of all AFREQ columns\n",
    "afreq_columns = [col for col in nri_df.columns if col.endswith('_AFREQ')]\n",
    "\n",
    "# fill missing values in these columns with 0\n",
    "nri_df[afreq_columns] = nri_df[afreq_columns].fillna(0)\n",
    "\n",
    "# check\n",
    "print(f\"missing values in AVLN_AFREQ after imputation: {nri_df['AVLN_AFREQ'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671b0b3",
   "metadata": {},
   "source": [
    "### Task 2 - SVI Data Cleaning\n",
    "#### 1. Import the SVI data. Ensure that the FIPS code is correctly identified as a string / character variable. Otherwise, the leading zeros will be removed. 1. Subset the SVI data to include only the following columns: ST, STATE, ST_ABBR, STCNTY, COUNTY, FIPS, LOCATION, AREA_SQMI, E_TOTPOP, EP_POV150, EP_UNEMP, EP_HBURD, EP_NOHSDP, EP_UNINSUR, EP_AGE65, EP_AGE17, EP_DISABL, EP_SNGPNT, EP_LIMENG, EP_MINRTY, EP_MUNIT, EP_MOBILE, EP_CROWD, EP_NOVEH, EP_GROUPQ, EP_NOINT, EP_AFAM, EP_HISP, EP_ASIAN, EP_AIAN, EP_NHPI, EP_TWOMORE, EP_OTHERRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2e2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ST    STATE ST_ABBR  STCNTY          COUNTY   FIPS  \\\n",
      "0   1  Alabama      AL    1001  Autauga County  01001   \n",
      "1   1  Alabama      AL    1003  Baldwin County  01003   \n",
      "2   1  Alabama      AL    1005  Barbour County  01005   \n",
      "3   1  Alabama      AL    1007     Bibb County  01007   \n",
      "4   1  Alabama      AL    1009   Blount County  01009   \n",
      "\n",
      "                  LOCATION    AREA_SQMI  E_TOTPOP  EP_POV150  ...  EP_NOVEH  \\\n",
      "0  Autauga County, Alabama   594.454786     58761       20.2  ...       4.0   \n",
      "1  Baldwin County, Alabama  1589.861817    233420       18.3  ...       2.3   \n",
      "2  Barbour County, Alabama   885.007619     24877       37.7  ...      11.7   \n",
      "3     Bibb County, Alabama   622.469286     22251       29.0  ...       7.5   \n",
      "4   Blount County, Alabama   644.890376     59077       22.9  ...       4.8   \n",
      "\n",
      "   EP_GROUPQ  EP_NOINT  EP_AFAM  EP_HISP  EP_ASIAN  EP_AIAN  EP_NHPI  \\\n",
      "0        0.9      10.9     19.6      3.2       1.1      0.1      0.0   \n",
      "1        1.5      10.9      8.3      4.8       0.9      0.2      0.0   \n",
      "2       12.0      31.8     46.9      4.8       0.5      0.3      0.0   \n",
      "3        6.4      20.2     20.7      2.9       0.3      0.1      0.0   \n",
      "4        1.0      16.9      1.2      9.7       0.2      0.1      0.2   \n",
      "\n",
      "   EP_TWOMORE  EP_OTHERRACE  \n",
      "0         3.3           0.2  \n",
      "1         3.1           0.4  \n",
      "2         1.8           1.2  \n",
      "3         1.7           0.1  \n",
      "4         2.8           0.1  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# path to your SVI file\n",
    "svi_path = '/Users/keith/Documents/code/Intro to ML 2025/data/raw/SVI_2022_US_county.csv'\n",
    "\n",
    "# import the SVI data w/ FIPS is a string\n",
    "svi_df = pd.read_csv(svi_path, dtype={'FIPS': str})\n",
    "\n",
    "# columns to keep\n",
    "svi_columns_to_keep = [\n",
    "    'ST', 'STATE', 'ST_ABBR', 'STCNTY', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
    "    'E_TOTPOP', 'EP_POV150', 'EP_UNEMP', 'EP_HBURD', 'EP_NOHSDP', 'EP_UNINSUR',\n",
    "    'EP_AGE65', 'EP_AGE17', 'EP_DISABL', 'EP_SNGPNT', 'EP_LIMENG', 'EP_MINRTY',\n",
    "    'EP_MUNIT', 'EP_MOBILE', 'EP_CROWD', 'EP_NOVEH', 'EP_GROUPQ', 'EP_NOINT',\n",
    "    'EP_AFAM', 'EP_HISP', 'EP_ASIAN', 'EP_AIAN', 'EP_NHPI', 'EP_TWOMORE', 'EP_OTHERRACE'\n",
    "]\n",
    "\n",
    "# make subset\n",
    "svi_subset = svi_df[svi_columns_to_keep]\n",
    "\n",
    "# check\n",
    "print(svi_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82b7be",
   "metadata": {},
   "source": [
    "#### 2. Create a table / dataframe that shows the number of missing values in each column. (Hint: if you wrote a function for Task 1, you can reuse it here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515f7da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST              0\n",
      "STATE           0\n",
      "ST_ABBR         0\n",
      "STCNTY          0\n",
      "COUNTY          0\n",
      "FIPS            0\n",
      "LOCATION        0\n",
      "AREA_SQMI       0\n",
      "E_TOTPOP        0\n",
      "EP_POV150       0\n",
      "EP_UNEMP        0\n",
      "EP_HBURD        0\n",
      "EP_NOHSDP       0\n",
      "EP_UNINSUR      0\n",
      "EP_AGE65        0\n",
      "EP_AGE17        0\n",
      "EP_DISABL       0\n",
      "EP_SNGPNT       0\n",
      "EP_LIMENG       0\n",
      "EP_MINRTY       0\n",
      "EP_MUNIT        0\n",
      "EP_MOBILE       0\n",
      "EP_CROWD        0\n",
      "EP_NOVEH        0\n",
      "EP_GROUPQ       0\n",
      "EP_NOINT        0\n",
      "EP_AFAM         0\n",
      "EP_HISP         0\n",
      "EP_ASIAN        0\n",
      "EP_AIAN         0\n",
      "EP_NHPI         0\n",
      "EP_TWOMORE      0\n",
      "EP_OTHERRACE    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# sum of nulls for each column\n",
    "missing_svi = svi_subset.isnull().sum()\n",
    "\n",
    "# check\n",
    "print(missing_svi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7db101",
   "metadata": {},
   "source": [
    "### Task 3 - Data Merging\n",
    "#### 1. Identify any FIPS codes that are present in the NRI data but not in the SVI data and vice versa. Describe any discrepancies and possible causes? What to these discrepancies, if any, mean for interpreting results based on the merged dataset moving forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2cd3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIPS codes in NRI only: {'72111', '72143', '78030', '72131', '72043', '72147', '72087', '72031', '72141', '72065', '72035', '72049', '09013', '72097', '69120', '72139', '69110', '72013', '72115', '72119', '72095', '72051', '60010', '09003', '72007', '72091', '72023', '60050', '09005', '72027', '72113', '72081', '09011', '72015', '72089', '72083', '78020', '09015', '72011', '72135', '72075', '72117', '72009', '72061', '72071', '72099', '72133', '72077', '72107', '72037', '72101', '78010', '09007', '72017', '72103', '72021', '72145', '72039', '72073', '72129', '72127', '72093', '72121', '72079', '72105', '72109', '72033', '72019', '72137', '09009', '72057', '72054', '72067', '72025', '72151', '66010', '72125', '72153', '72047', '72123', '72059', '60020', '69100', '72055', '72085', '09001', '72029', '72063', '72069', '72053', '72149', '72001', '72005', '72003', '72041', '72045'}\n",
      "Count: 96\n",
      "\n",
      "FIPS codes in SVI only: {'09120', '09160', '09140', '09180', '09110', '09170', '09150', '09190', '09130'}\n",
      "Count: 9\n"
     ]
    }
   ],
   "source": [
    "# get the set of FIPS codes from each df\n",
    "nri_fips = set(nri_df['STCOFIPS'])\n",
    "svi_fips = set(svi_df['FIPS'])\n",
    "\n",
    "# find FIPS codes in NRI but not in SVI\n",
    "nri_only = nri_fips - svi_fips\n",
    "print(f\"FIPS codes in NRI only: {nri_only}\")\n",
    "print(f\"Count: {len(nri_only)}\\n\")\n",
    "\n",
    "\n",
    "# find FIPS codes in SVI but not in NRI\n",
    "svi_only = svi_fips - nri_fips\n",
    "print(f\"FIPS codes in SVI only: {svi_only}\")\n",
    "print(f\"Count: {len(svi_only)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453de92b",
   "metadata": {},
   "source": [
    "#### The two datasets don't perfectly align because the SVI data includes Puerto Rico while the NRI does not, and the NRI also uses a few outdated county codes. When these files are merged, you get incomplete rows for mismatched counties, and data for specific locations like military bases is simply included within their surrounding county's record. This means your results could be misleading unless you first harmonize the county codes and account for these geographic differences in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2c93b",
   "metadata": {},
   "source": [
    "#### 2. Merge the NRI and SVI data on the FIPS code. Use an outer join to keep all counties in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537f43ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 498)\n"
     ]
    }
   ],
   "source": [
    "# rename columns before merging\n",
    "nri_cleaned = nri_df.rename(columns={'STCOFIPS': 'FIPS'})\n",
    "svi_cleaned = svi_subset # Already subsetted in Task 2\n",
    "\n",
    "# outer merge\n",
    "merged_df = pd.merge(\n",
    "    nri_cleaned,\n",
    "    svi_cleaned,\n",
    "    on='FIPS',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# show size of merged df\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c89995",
   "metadata": {},
   "source": [
    "#### 3. Create a table / dataframe that shows the number of missing values in each column of the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e567c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OID_             9\n",
      "NRI_ID           9\n",
      "STATE_x          9\n",
      "STATEABBRV       9\n",
      "STATEFIPS        9\n",
      "                ..\n",
      "EP_ASIAN        96\n",
      "EP_AIAN         96\n",
      "EP_NHPI         96\n",
      "EP_TWOMORE      96\n",
      "EP_OTHERRACE    96\n",
      "Length: 498, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get missing value counts for the merged df\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9dd2b",
   "metadata": {},
   "source": [
    "### Task 4 - Data Analysis\n",
    "#### 1. For each numerical variable in the merged dataset, plot a histogram showing the distribution of values. (Hint: write a function to make the histogram for a single variable, then use a loop or apply function to make the histograms for all numerical variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a093d8c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m os.makedirs(\u001b[33m'\u001b[39m\u001b[33m/Users/keith/Documents/code/Intro to ML 2025/data/processed\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Save the merged and cleaned dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mmerged_df\u001b[49m.to_csv(\u001b[33m'\u001b[39m\u001b[33mdata/processed/processed_data.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved processed data to: data/processed/processed_data.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# select only numerical columns for plotting\n",
    "#numerical_cols = merged_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# figure to hold the plots\n",
    "#plt.figure(figsize=(20, 200))\n",
    "\n",
    "#for i, col in enumerate(numerical_cols):\n",
    "    #plt.subplot(len(numerical_cols) // 4 + 1, 4, i + 1)\n",
    "    #sns.histplot(merged_df[col], kde=False, bins=50)\n",
    "    #plt.title(col)\n",
    "    #plt.xlabel('')\n",
    "    #plt.ylabel('')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "import os\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('/Users/keith/Documents/code/Intro to ML 2025/data/processed', exist_ok=True)\n",
    "\n",
    "# Save the merged and cleaned dataset\n",
    "merged_df.to_csv('data/processed/processed_data.csv', index=False)\n",
    "print(\"Saved processed data to: data/processed/processed_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
